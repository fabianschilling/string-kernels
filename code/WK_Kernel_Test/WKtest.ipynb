{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Description: Test for computing WK kernel\n",
    "# Author: J Manttari\n",
    "# Notes: clean_input_docs, doc_to_words and remove_nltk_stopwords \n",
    "#         can be disregarded if using only skilearn methods.\n",
    "#-----------------------------------------------------------------\n",
    "import pandas as pd       \n",
    "from bs4 import BeautifulSoup             \n",
    "import re\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_input_docs(raw_docs): \n",
    "    print \"Cleaning and parsing the docs...\\n\"\n",
    "    clean_docs = []\n",
    "    num_docs = len(raw_docs)\n",
    "    for i in xrange( 0, num_docs ):\n",
    "        # Status message in case it takes a while\n",
    "        if( (i+1)%1000 == 0 ):\n",
    "            print \"doc %d of %d\\n\" % ( i+1, num_docs )                                                                    \n",
    "        clean_docs.append( doc_to_words( raw_docs[i] ))\n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_to_words( raw_doc ):\n",
    "\n",
    "    doc_text = BeautifulSoup(raw_doc).get_text() \n",
    "    \n",
    "    # Remove non-letters     \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", doc_text) \n",
    "    \n",
    "    #Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "\n",
    "    #nltk english stopwords are less picky than sci-kit. Which should we use?\n",
    "    words = remove_nltk_stopwords(words)\n",
    "    \n",
    "    return( \" \".join( words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nltk_stopwords( words ):\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "num_docs = docs[\"review\"].size\n",
    "\n",
    "#clean input either with beautiful soup + regex \n",
    "#clean_docs = clean_input_docs(docs[\"review\"][0:100])\n",
    "\n",
    "# or trust sklearn to do it later\n",
    "clean_docs = docs[\"review\"][0:100] #slice of 100 is just to test without using to much computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "0\n",
      "done\n",
      "(100, 4176)\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "\n",
    "#defaultanalyzer \"word\" removes non-chars in preprocessing and tokenizes words. does not remove \"markup tokens\"\n",
    "#stop_words should be \"english\" if not using clean_input_docs()\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = \"english\",   \\\n",
    "                             max_features = 5000) #paper didn't say anything about max feature count? what should we have?\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_docs)\n",
    "train_data_features = train_data_features.toarray()\n",
    "print train_data_features[0][0]\n",
    "#print train_data_features\n",
    "#print \"idf...\"\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(train_data_features)\n",
    "tfidf = tfidf.toarray() \n",
    "#print tfidf\n",
    "print \"done\"\n",
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 100 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute Kernel Matrix. This takes ~15 minutes for all 25 000 doc with 5 000 features\n",
    "Kmat = np.ones((len(tfidf),len(tfidf)))\n",
    "\n",
    "for i in xrange( 0, len(tfidf) ):\n",
    "    if( (i+1)%100 == 0 ):\n",
    "        print \"row %d of %d\\n\" % ( i+1, num_docs )       \n",
    "    for j in xrange(0,len(tfidf)):\n",
    "        Kmat[i][j] = np.dot(tfidf[i], tfidf[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the Kernel matrix for use later as it can take a while to compute.\n",
    "with open('WKKernelMat.pickle', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(Kmat, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
