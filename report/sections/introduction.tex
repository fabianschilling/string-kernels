\section{Introduction} \label{sec:intro}

Many learning systems such as neural networks and decision trees rely on input data being transformed into feature vectors before being analyzed and used for prediction \cite{word2vec, glove}. However, the construction of these feature vectors can prove to be a difficult task in certain cases, such as those involving text documents or biosequences as input data. The extraction of useful features often requires extensive domain knowledge and expertise.

It is possible to circumvent these issues regarding explicit feature extraction with the use of kernel-based learning methods (KM) \cite{vapnik}. These methods incorporate the use of a kernel function which defines the inner product between data points in a higher dimensional space without the need to express the datapoints explicitly as feature vectors. Learning algorithms such as support vector machines (SVM), $k$-nearest-neighbour ($k$NN) and perceptrons can then be used by expressing the data as these inner products. Kernel methods are a strong candidate for classification and especially text classification tasks, as they are not dependant on the dimensionality of the feature space which makes them suitable for use with a high dimensionality feature space.

Previous methods for document topic classification \cite{wk} have used SVMs in conjunction with mapping the text documents to word-based feature vectors. The method presented in this paper instead uses a specific string kernel for mapping and treats the documents simply as sequences of characters. The kernel defines inner products between documents in the feature space of all non-contiguous character subsequences of length $k$, resulting in higher inner products for documents with more subsequences in common.

The remainder of this paper is structured as follows. Section \ref{sec:method} describes the kernels used for experiments in detail and outlines our evaluation methodology. Section \ref{sec:results} presents the results obtained during the experiments on a publicly available dataset. Section \ref{sec:discussion} reviews our findings and argues for and against the different string kernels. We end our paper with a conclusion in section \ref{sec:conclusions}.
