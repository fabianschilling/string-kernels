\section{Introduction} \label{sec:intro}
Many learning systems [TODO: maybe give a list with some references] rely on input data being transformed into feature vectors before being analyzed and used for prediction. The construction of these feature vectors can however prove to be a difficult task in certain cases, such as those involving text documents or biosequences as input data. (where extensive domain knowledge is required or information loss due to feature extraction easily occurs).
It is possible to circumvent these issues regarding explicit feature extraction with the use of Kernel-based learning methods (KMs) \cite{Vapnik95}. These methods incorporate the use of a kernel function which defines the inner product between datapoints in a higher dimensional space without the need to express the datapoints explicitly as feature vectors. Learning algorithms such as Support Vector Machines (SVMs), k-Nearest-Neighbour (KNN) and Perceptrons can then be used by expressing the data as these inner products. KMs are a strong candidate for classification and especially text classification tasks, as they are not dependant on the dimensionality of the feature space which makes them suitable for use with a high dimensionality feature space. 

Previous methods for document topic classification \cite{Joachims1998} have used SVMs in conjunction with mapping the text documents to word-based feature vectors. The method presented in this paper instead uses a specific string kernel for mapping and treats the documents simply as sequences of characters. The kernel defines inner products between documents in the feature space of all non-contiguous character subsequences of length k, resulting in higher inner products for documents with more subsequences in common. 

[TODO: do we have space for a background section? And is it necessary? (ppl prolly know about SVMs alreadyn and def. know about kernels)]

The following section gives a brief background to SVMs and their use in conjunction with the method presented in this paper. In section \ref{sec:method} we explain the construction of the used kernel in more detail, as well as present a dynamic programming technique for decreasing the complexity of its evaluation. The performance of the proposed method is then compared to that of using a standard word kernel and one based on n-grams, with results shown in section \ref{sec:results}. These results are finally discussed in section \ref{sec:discussion}, followed by a conclusion of the paper in section \ref{sec:conclusions}.
