\section{Introduction} \label{sec:intro}

Many learning systems such as neural networks and decision trees rely on input data being transformed into feature vectors before being analyzed and used for prediction \cite{word2vec, glove}. However, the construction of these feature vectors can prove to be a difficult task in certain cases, such as those involving text documents or biosequences as input data. The extraction of useful features often requires extensive domain knowledge and expertise.

It is possible to circumvent these issues regarding explicit feature extraction with the use of kernel-based learning methods (KM) \cite{vapnik}. These methods incorporate the use of a kernel function which defines the inner product between data points in a higher dimensional space without the need to express the datapoints explicitly as feature vectors. Learning algorithms such as support vector machines (SVM), $k$-nearest-neighbour ($k$NN) and perceptrons can then be used by expressing the data as these inner products. Kernel methods are a strong candidate for classification and especially text classification tasks, as they are not dependant on the dimensionality of the feature space which makes them suitable for use with a high dimensionality feature space.

Previous methods for document topic classification \cite{wk} have used SVMs in conjunction with mapping the text documents to word-based feature vectors. The method presented in this paper instead uses a specific string kernel for mapping and treats the documents simply as sequences of characters. The kernel defines inner products between documents in the feature space of all non-contiguous character subsequences of length k, resulting in higher inner products for documents with more subsequences in common.

The following section gives a brief background to SVMs and their use in conjunction with the method presented in this paper. In section \ref{sec:method} we explain the construction of the used kernel in more detail, as well as present a dynamic programming technique for decreasing the complexity of its evaluation. The performance of the proposed method is then compared to that of using a standard word kernel and one based on n-grams, with results shown in section \ref{sec:results}. These results are finally discussed in section \ref{sec:discussion}, followed by a conclusion of the paper in section \ref{sec:conclusions}.
